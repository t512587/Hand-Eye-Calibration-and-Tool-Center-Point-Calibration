import numpy as np
import cv2
import json
import os
import math
import pyrealsense2 as rs
from pymycobot.elephantrobot import ElephantRobot
from scipy.spatial.transform import Rotation as R
import time
from collections import deque

# === æ£‹ç›¤æ ¼è¨­å®š ===
CHESSBOARD_SIZE = (9, 6)  # å…§è§’é»æ•¸é‡ (åˆ—, è¡Œ)
SQUARE_SIZE = 0.025  # æ¯å€‹æ–¹æ ¼çš„å¯¦éš›å¤§å°ï¼Œå–®ä½: å…¬å°º (25mm)

# === æ¡é›†æ•¸æ“š JSON æª”æ¡ˆè·¯å¾‘ ===
CALIBRATION_DATA_JSON_PATH = r"C:\Users\user\Downloads\Hand-Eye-Calibration-main\eye_in_hand\handeye_records\handeye_chessboard_20260204_185505.json"


# ============================================================
# æ‰‹çœ¼æ¨™å®šè¨ˆç®—ç›¸é—œå‡½æ•¸
# ============================================================

def euler_to_rotation_matrix(roll, pitch, yaw, seq="xyz"):
    """å°‡æ­æ‹‰è§’è½‰æ›ç‚ºæ—‹è½‰çŸ©é™£"""
    roll = math.radians(roll)
    pitch = math.radians(pitch)
    yaw = math.radians(yaw)

    Rx = np.array([[1, 0, 0],
                   [0, math.cos(roll), -math.sin(roll)],
                   [0, math.sin(roll), math.cos(roll)]])
    Ry = np.array([[math.cos(pitch), 0, math.sin(pitch)],
                   [0, 1, 0],
                   [-math.sin(pitch), 0, math.cos(pitch)]])
    Rz = np.array([[math.cos(yaw), -math.sin(yaw), 0],
                   [math.sin(yaw), math.cos(yaw), 0],
                   [0, 0, 1]])

    if seq == "xyz":
        return Rz @ Ry @ Rx
    elif seq == "zyx":
        return Rx @ Ry @ Rz
    else:
        raise ValueError("ä¸æ”¯æ´çš„æ—‹è½‰é †åº")


def to_homogeneous(R, t):
    """å°‡æ—‹è½‰çŸ©é™£å’Œå¹³ç§»å‘é‡è½‰æ›ç‚ºé½Šæ¬¡çŸ©é™£"""
    H = np.eye(4)
    H[:3, :3] = R
    H[:3, 3:] = t
    return H


def is_rotation_matrix(R):
    """æª¢æŸ¥ä¸€å€‹çŸ©é™£æ˜¯å¦ç‚ºæœ‰æ•ˆçš„æ—‹è½‰çŸ©é™£"""
    Rt = np.transpose(R)
    should_be_identity = Rt @ R
    I = np.identity(3, dtype=R.dtype)
    return np.linalg.norm(I - should_be_identity) < 1e-6


def calculate_handeye_calibration(json_path):
    """
    å¾ JSON æª”æ¡ˆè®€å–æ¡é›†æ•¸æ“šä¸¦è¨ˆç®—æ‰‹çœ¼æ¨™å®š (ä½¿ç”¨ TSAI æ–¹æ³•)
    
    Args:
        json_path: æ¡é›†æ•¸æ“šçš„ JSON æª”æ¡ˆè·¯å¾‘
        
    Returns:
        np.array: 4x4 ç›¸æ©Ÿåˆ°å¤¾çˆªçš„è®Šæ›çŸ©é™£ (å–®ä½: m) âœ… æ”¹ç‚ºç±³
    """
    print(f"\n{'='*60}")
    print("æ‰‹çœ¼æ¨™å®šè¨ˆç®— (TSAI æ–¹æ³•)")
    print(f"{'='*60}")
    print(f"æ•¸æ“šæª”æ¡ˆ: {json_path}")
    
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•¸æ“šæª”æ¡ˆ: {json_path}")
    
    # è¼‰å…¥ JSON æ•¸æ“š
    with open(json_path, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)
    
    print(f"âœ“ è¼‰å…¥ {len(raw_data)} ç­†æ•¸æ“š")
    
    # === è½‰æ›æ•¸æ“šæ ¼å¼ & å–®ä½ ===
    # âœ… çµ±ä¸€ï¼šå°‡æ‰€æœ‰é•·åº¦å–®ä½å¾æ¯«ç±³ (mm) è½‰æ›ç‚ºå…¬å°º (m)
    data = []
    for d in raw_data:
        # æ”¯æ´æ–°èˆŠå…©ç¨®æ ¼å¼
        if "chessboard_data" in d:
            # æ–°æ ¼å¼ï¼šä½¿ç”¨ chessboard_data
            tvec_mm = d["chessboard_data"]["translation_mm"]
            rvec = d["chessboard_data"]["rotation_vector"]
            robot_pose_key = "robot_coords"
        elif "aruco_tvec" in d:
            # èˆŠæ ¼å¼ï¼šArUco æ¨™è¨˜
            tvec_mm = d["aruco_tvec"]
            rvec = d["aruco_rvec"]
            robot_pose_key = "robot_pose_at_detect"
        else:
            print(f"âš ï¸ è·³éä¸æ”¯æ´çš„æ•¸æ“šæ ¼å¼: {d.keys()}")
            continue
        
        aruco_tvec = [x / 1000 for x in tvec_mm]  # mm to m
        robot_pose = d[robot_pose_key].copy()
        robot_pose[:3] = [x / 1000 for x in robot_pose[:3]]  # mm to m
        
        data.append({
            "aruco_tvec": aruco_tvec,
            "aruco_rvec": rvec,
            "robot_pose_at_detect": robot_pose
        })
    
    print("âœ… è³‡æ–™å–®ä½è½‰æ›å®Œæˆ (mm -> m)ã€‚")
    
    # === å»ºç«‹æ—‹è½‰å’Œå¹³ç§»é™£åˆ— ===
    R_gripper2base, t_gripper2base = [], []
    R_target2cam, t_target2cam = [], []

    for d in data:
        t_g2b = np.array(d["robot_pose_at_detect"][:3]).reshape((3, 1))
        rpy_g2b = d["robot_pose_at_detect"][3:]
        R_g2b = euler_to_rotation_matrix(*rpy_g2b, seq="xyz")
        R_gripper2base.append(R_g2b)
        t_gripper2base.append(t_g2b)

        rvec_t2c = np.array(d["aruco_rvec"]).reshape((3, 1))
        tvec_t2c = np.array(d["aruco_tvec"]).reshape((3, 1))
        R_t2c, _ = cv2.Rodrigues(rvec_t2c)
        R_target2cam.append(R_t2c)
        t_target2cam.append(tvec_t2c)
    
    # === ä½¿ç”¨ TSAI æ–¹æ³•è¨ˆç®— ===
    print("âš™ï¸ åŸ·è¡Œ TSAI æ‰‹çœ¼æ¨™å®š...")
    R_cam2gripper, t_cam2gripper = cv2.calibrateHandEye(
        R_gripper2base,
        t_gripper2base,
        R_target2cam,
        t_target2cam,
        method=cv2.CALIB_HAND_EYE_TSAI
    )
    
    H_cam2gripper = to_homogeneous(R_cam2gripper, t_cam2gripper)
    
    print("âœ… æ‰‹çœ¼æ ¡æ­£å®Œæˆï¼")
    
    # === é¡¯ç¤ºçµæœ ===
    print("\n--- æ ¡æ­£çµæœ ---")
    print("ğŸ“·â¡ï¸ğŸ¤– ç›¸æ©Ÿåˆ°æœ«ç«¯ (Cam to Gripper) é½Šæ¬¡è½‰æ›çŸ©é™£ H_cam2gripperï¼š")
    print(H_cam2gripper)
    print(f"æ˜¯å¦ç‚ºæœ‰æ•ˆçš„æ—‹è½‰çŸ©é™£: {is_rotation_matrix(R_cam2gripper)}")
    print(f"å¹³ç§»å‘é‡ (m): {t_cam2gripper.ravel()}")
    
    # === é©—è­‰çµæœ ===
    all_positions = []
    for i in range(len(R_gripper2base)):
        H_gripper2base = to_homogeneous(R_gripper2base[i], t_gripper2base[i])
        H_target2cam = to_homogeneous(R_target2cam[i], t_target2cam[i])
        H_target2base = H_gripper2base @ H_cam2gripper @ H_target2cam
        pos = (H_target2base @ np.array([0, 0, 0, 1]).reshape((4, 1))).ravel()[:3]
        all_positions.append(pos)
    
    avg_pos = np.mean(all_positions, axis=0)
    std_dev = np.std(all_positions, axis=0)
    euclidean_std = np.linalg.norm(std_dev)
    
    print(f"\næ‰€æœ‰æ¨™å®šé»åœ¨åŸºåº§åº§æ¨™çš„å¹³å‡ä½ç½® (m): {avg_pos}")
    print(f"æ‰€æœ‰æ¨™å®šé»åœ¨åŸºåº§åº§æ¨™çš„æ¨™æº–å·® (m) (è¶Šå°è¶Šå¥½): {std_dev}")
    print(f"æ­æ°è·é›¢æ¨™æº–å·® (m): {euclidean_std:.6f}")
    print(f"âœ“ æ¨™å®šå®Œæˆï¼Œç²¾åº¦: {euclidean_std * 1000:.2f} mm")
    
    # âœ… ä¿æŒç±³å–®ä½ï¼Œä¸è½‰æ›ç‚ºæ¯«ç±³
    return H_cam2gripper  # è¿”å›ç±³å–®ä½çš„çŸ©é™£


# ============================================================
# å¯¦æ™‚åº§æ¨™è½‰æ›ç³»çµ±
# ============================================================

class RealTimeCoordinateTransform:
    def __init__(self, camera_to_gripper_matrix, robot_ip="192.168.50.123", robot_port=5001):
        """
        åˆå§‹åŒ–å¯¦æ™‚åº§æ¨™è½‰æ›ç³»çµ±
        
        Args:
            camera_to_gripper_matrix: ç›¸æ©Ÿåˆ°å¤¾çˆªçš„è®Šæ›çŸ©é™£ (å–®ä½: m) âœ…
        """
        self.T_camera_gripper = np.array(camera_to_gripper_matrix)  # å–®ä½: ç±³
        
        self.robot_ip = robot_ip
        self.robot_port = robot_port
        self.robot = None
        self.robot_connected = False
        
        self.camera_matrix = None
        self.dist_coeffs = np.zeros((5, 1))
        
        self.chessboard_size = CHESSBOARD_SIZE
        self.square_size = SQUARE_SIZE
        self.objp = self._create_chessboard_points()
        
        self.criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
        
        self.pipeline = None
        self.current_transform = None
        
        print("å¯¦æ™‚åº§æ¨™è½‰æ›ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
        print(f"âš™ï¸ T_camera_gripper å–®ä½: ç±³ (m)")
    
    def _create_chessboard_points(self):
        objp = np.zeros((self.chessboard_size[0] * self.chessboard_size[1], 3), np.float32)
        objp[:, :2] = np.mgrid[0:self.chessboard_size[0], 0:self.chessboard_size[1]].T.reshape(-1, 2)
        objp *= self.square_size
        return objp
    
    def connect_robot(self):
        try:
            self.robot = ElephantRobot(self.robot_ip, self.robot_port)
            self.robot.start_client()
            self.robot_connected = True
            print(f"âœ“ æ©Ÿæ¢°æ‰‹è‡‚é€£æ¥æˆåŠŸ: {self.robot_ip}:{self.robot_port}")
            return True
        except Exception as e:
            print(f"âš ï¸ æ©Ÿæ¢°æ‰‹è‡‚é€£æ¥å¤±æ•—: {e}")
            self.robot_connected = False
            return False
    
    def init_camera(self):
        try:
            self.pipeline = rs.pipeline()
            config = rs.config()
            config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
            
            profile = self.pipeline.start(config)
            
            color_stream = profile.get_stream(rs.stream.color)
            intrinsics = color_stream.as_video_stream_profile().get_intrinsics()
            
            self.camera_matrix = np.array([
                [intrinsics.fx, 0, intrinsics.ppx],
                [0, intrinsics.fy, intrinsics.ppy],
                [0, 0, 1]
            ])
            
            if len(intrinsics.coeffs) >= 5:
                self.dist_coeffs = np.array(intrinsics.coeffs[:5]).reshape(5, 1)
            
            print(f"\nâœ“ ç›¸æ©Ÿå…§åƒ: fx={intrinsics.fx:.1f}, fy={intrinsics.fy:.1f}")
            print("âœ“ RealSenseç›¸æ©Ÿåˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ ç›¸æ©Ÿåˆå§‹åŒ–å¤±æ•—: {e}")
            return False
    
    def invert_transform(self, T):
        """è¨ˆç®—è®Šæ›çŸ©é™£çš„é€†"""
        R_mat = T[:3, :3]
        t = T[:3, 3]
        R_inv = R_mat.T
        t_inv = -R_inv @ t
        T_inv = np.eye(4)
        T_inv[:3, :3] = R_inv
        T_inv[:3, 3] = t_inv
        return T_inv
    
    def transform_point(self, T, P):
        """ä½¿ç”¨è®Šæ›çŸ©é™£è½‰æ› 3D é»"""
        P_h = np.ones(4)
        P_h[:3] = P
        P_transformed = T @ P_h
        return P_transformed[:3]
    
    def camera_to_base_transform(self, rvec, tvec, gripper_pose):
        """
        å°‡æ£‹ç›¤æ ¼å¾ç›¸æ©Ÿåº§æ¨™ç³»è½‰æ›åˆ°åŸºåº§åº§æ¨™ç³»
        âœ… çµ±ä¸€ä½¿ç”¨ç±³ (m) é€²è¡Œè¨ˆç®—
        
        Args:
            rvec: æ—‹è½‰å‘é‡ (from solvePnP)
            tvec: å¹³ç§»å‘é‡ (from solvePnP), å–®ä½ m
            gripper_pose: [x, y, z, rx, ry, rz], ä½ç½®å–®ä½ mm
        
        Returns:
            æ£‹ç›¤æ ¼åœ¨åŸºåº§åº§æ¨™ç³»çš„ä½ç½® [x, y, z] (m) âœ… æ”¹ç‚ºè¿”å›ç±³
        """
        x, y, z, rx, ry, rz = gripper_pose
        
        # æ­¥é©Ÿ 1: å»ºç«‹ T_base_gripper (å–®ä½: m)
        translation = np.array([x, y, z]) / 1000.0  # âœ… mm â†’ m
        rotation = R.from_euler('xyz', [rx, ry, rz], degrees=True)
        rotation_matrix = rotation.as_matrix()
        
        T_base_gripper = np.eye(4)
        T_base_gripper[:3, :3] = rotation_matrix
        T_base_gripper[:3, 3] = translation  # å–®ä½: m
        
        # æ­¥é©Ÿ 2: è¨ˆç®— T_gripper_camera = inverse(T_camera_gripper)
        T_gripper_camera = self.invert_transform(self.T_camera_gripper)  # å–®ä½: m
        
        # æ­¥é©Ÿ 3: å»ºç«‹ T_camera_target (æ£‹ç›¤æ ¼åœ¨ç›¸æ©Ÿåº§æ¨™ç³», å–®ä½: m)
        R_camera_target, _ = cv2.Rodrigues(rvec)
        t_camera_target = tvec.flatten()  # âœ… ä¿æŒç±³å–®ä½ï¼Œä¸è½‰æ›
        
        T_camera_target = np.eye(4)
        T_camera_target[:3, :3] = R_camera_target
        T_camera_target[:3, 3] = t_camera_target  # å–®ä½: m
        
        # æ­¥é©Ÿ 4: å®Œæ•´è®Šæ›éˆï¼ˆå…¨éƒ¨ä½¿ç”¨ç±³ï¼‰
        # T_base_target = T_base_gripper @ T_gripper_camera @ T_camera_target
        T_base_target = T_base_gripper @ T_gripper_camera @ T_camera_target
        
        # æ­¥é©Ÿ 5: æå–æ£‹ç›¤æ ¼åŸé»åœ¨åŸºåº§åº§æ¨™ç³»çš„ä½ç½®
        target_origin = np.array([0, 0, 0, 1])
        base_position = (T_base_target @ target_origin)[:3]  # å–®ä½: m
        
        return base_position
    
    def detect_chessboard(self, color_image):
        """æª¢æ¸¬æ£‹ç›¤æ ¼"""
        gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)
        
        ret, corners = cv2.findChessboardCorners(
            gray, self.chessboard_size,
            cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_NORMALIZE_IMAGE + cv2.CALIB_CB_FAST_CHECK
        )
        
        if ret:
            corners_refined = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), self.criteria)
            
            success, rvec, tvec = cv2.solvePnP(
                self.objp, corners_refined, self.camera_matrix, self.dist_coeffs,
                flags=cv2.SOLVEPNP_ITERATIVE
            )
            
            if success:
                camera_pos = tvec.flatten()  # âœ… å–®ä½: m
                return {
                    'detected': True,
                    'camera_position': camera_pos,
                    'corners': corners_refined,
                    'rvec': rvec,
                    'tvec': tvec
                }
        
        return {'detected': False}
    
    def draw_results(self, color_image, detection_result):
        """ç¹ªè£½æª¢æ¸¬çµæœ"""
        if detection_result['detected']:
            cv2.drawChessboardCorners(
                color_image, self.chessboard_size, 
                detection_result['corners'], True
            )
            
            cv2.drawFrameAxes(
                color_image, self.camera_matrix, self.dist_coeffs,
                detection_result['rvec'], detection_result['tvec'],
                self.square_size * 3
            )
            
            # âœ… é¡¯ç¤ºæ™‚è½‰æ›ç‚ºæ¯«ç±³
            cam_pos = detection_result['camera_position'] * 1000  # m â†’ mm
            cv2.putText(color_image, f"Cam: ({cam_pos[0]:.0f}, {cam_pos[1]:.0f}, {cam_pos[2]:.0f}) mm",
                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
            
            if self.current_transform:
                base_pos = self.current_transform['base_position'] * 1000  # m â†’ mm
                cv2.putText(color_image, f"Base: ({base_pos[0]:.0f}, {base_pos[1]:.0f}, {base_pos[2]:.0f}) mm",
                           (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
            
            cv2.putText(color_image, "Chessboard DETECTED", (10, 90),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        else:
            cv2.putText(color_image, "Chessboard NOT found", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        
        # é¡¯ç¤ºæŒ‰éµèªªæ˜
        cv2.putText(color_image, "C: Print coords  Q: Quit", (10, color_image.shape[0] - 20),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    
    def run(self):
        """åŸ·è¡Œå¯¦æ™‚åº§æ¨™è½‰æ›"""
        if not self.init_camera():
            return
        
        self.connect_robot()
        
        print(f"\n{'='*60}")
        print("=== å¯¦æ™‚åº§æ¨™è½‰æ›ç³»çµ±å•Ÿå‹• ===")
        print("å–®ä½ç³»çµ±: å…§éƒ¨è¨ˆç®—ä½¿ç”¨ç±³ (m), é¡¯ç¤ºä½¿ç”¨æ¯«ç±³ (mm)")
        print("='*60}")
        print("æŒ‰ 'C' éµ: è¼¸å‡ºç•¶å‰åº§æ¨™")
        print("æŒ‰ 'Q' éµ: é€€å‡º")
        print("=" * 60)
        
        try:
            while True:
                frames = self.pipeline.wait_for_frames()
                color_frame = frames.get_color_frame()
                if not color_frame:
                    continue
                
                color_image = np.asanyarray(color_frame.get_data())
                
                detection_result = self.detect_chessboard(color_image)
                
                if self.robot_connected and detection_result['detected']:
                    try:
                        current_gripper_pose = self.robot.get_coords()
                        
                        # ä½¿ç”¨å®Œæ•´çš„ rvec å’Œ tvec é€²è¡Œè½‰æ›ï¼ˆå–®ä½: mï¼‰
                        base_pos = self.camera_to_base_transform(
                            detection_result['rvec'],
                            detection_result['tvec'],
                            current_gripper_pose
                        )
                        
                        camera_pos = detection_result['camera_position']  # å–®ä½: m
                        
                        self.current_transform = {
                            'camera_position': camera_pos,  # m
                            'base_position': base_pos  # m
                        }
                    except Exception as e:
                        print(f"âš ï¸ åº§æ¨™è½‰æ›éŒ¯èª¤: {e}")
                
                self.draw_results(color_image, detection_result)
                
                cv2.imshow('Coordinate Transform', color_image)
                
                key = cv2.waitKey(1) & 0xFF
                
                if key == ord('q') or key == ord('Q'):
                    break
                    
                elif key == ord('c') or key == ord('C'):
                    if self.current_transform:
                        # âœ… é¡¯ç¤ºæ™‚è½‰æ›ç‚ºæ¯«ç±³
                        cam = self.current_transform['camera_position'] * 1000  # m â†’ mm
                        base = self.current_transform['base_position'] * 1000  # m â†’ mm
                        print(f"\nç›¸æ©Ÿåº§æ¨™: ({cam[0]:.1f}, {cam[1]:.1f}, {cam[2]:.1f}) mm")
                        print(f"åŸºåº§åº§æ¨™: ({base[0]:.1f}, {base[1]:.1f}, {base[2]:.1f}) mm")
                    else:
                        print("âš ï¸ æœªæª¢æ¸¬åˆ°æ£‹ç›¤æ ¼")
        
        except KeyboardInterrupt:
            print("\nâ›” ä¸­æ–·...")
        
        finally:
            if self.pipeline:
                self.pipeline.stop()
            cv2.destroyAllWindows()
            print("âœ… ç³»çµ±å·²é—œé–‰")


# ============================================================
# ä¸»ç¨‹å¼
# ============================================================

def main():
    print("=" * 60)
    print("å¯¦æ™‚ç›¸æ©Ÿåˆ°æ©Ÿæ¢°æ‰‹è‡‚åº§æ¨™è½‰æ›ç³»çµ±")
    print("å–®ä½ç³»çµ±: çµ±ä¸€ä½¿ç”¨ç±³ (m) é€²è¡Œè¨ˆç®—")
    print("=" * 60)
    
    # === è¼¸å…¥æ¡é›†æ•¸æ“š JSON æª”æ¡ˆè·¯å¾‘ ===
    json_path = input(f"\nè«‹è¼¸å…¥æ•¸æ“š JSON è·¯å¾‘ (Enter ä½¿ç”¨é è¨­): ").strip()
    if not json_path:
        json_path = CALIBRATION_DATA_JSON_PATH
    
    # === è¨ˆç®—æ‰‹çœ¼æ¨™å®š ===
    try:
        T_camera_gripper = calculate_handeye_calibration(json_path)
        print(f"\nâœ… æ‰‹çœ¼æ¨™å®šçŸ©é™£å·²è¨ˆç®—å®Œæˆ (å–®ä½: m)")
    except Exception as e:
        print(f"\nâŒ æ¨™å®šå¤±æ•—: {e}")
        return
    
    # === æ©Ÿå™¨äºº IP ===
    robot_ip = input("\næ©Ÿå™¨äºº IP (Enter ä½¿ç”¨ 192.168.50.123): ").strip()
    if not robot_ip:
        robot_ip = "192.168.50.123"
    
    # === å•Ÿå‹• ===
    choice = input("\nå•Ÿå‹•å¯¦æ™‚ç³»çµ±ï¼Ÿ(y/n): ").strip().lower()
    if choice != 'y':
        return
    
    transform_system = RealTimeCoordinateTransform(
        camera_to_gripper_matrix=T_camera_gripper,
        robot_ip=robot_ip,
        robot_port=5001
    )
    
    transform_system.run()


if __name__ == "__main__":
    main()